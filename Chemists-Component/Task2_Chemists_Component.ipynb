{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import rdkit, scipy, sklearn\n",
    "import tensorflow as tf\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import DataStructs, Descriptors\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit import Avalon\n",
    "from rdkit.Avalon import pyAvalonTools\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, matthews_corrcoef, recall_score, precision_score,f1_score\n",
    "\n",
    "\n",
    "from load_data import load_data\n",
    "from write import write_config_file, write_query_to_csv, write_runs_sh, write_idx, write_training_data, write_sample_file, write_run_sample\n",
    "from acquisition import select_query\n",
    "from models import Tanimoto_model\n",
    "from query import query\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import *\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gpflow\n",
    "from gpflow.utilities import positive\n",
    "from gpflow.utilities.ops import broadcasting_elementwise\n",
    "from gpflow.mean_functions import Constant\n",
    "import tensorflow as tf\n",
    "import reinvent_scoring\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 1 # options: numbers 1-5 that define which seed is used in user demo\n",
    "\n",
    "seeds = [1718, 1896, 3975, 8355, 9774] \n",
    "seed = seeds[experiment-1]\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "acquisition='thompson' #options: 'thompson', 'uncertainty', 'random', 'greedy'\n",
    "READ_ONLY = False # Use 'True' to playback an existing experiment (reads queries and feedback from files instead of using the algorithm)\n",
    "usecase = 'drd2'\n",
    "simulated_human = True\n",
    "verbose = False # print out details during interaction\n",
    "loop, last_job=0, None # Do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_usecase(usecase):\n",
    "    usecase_params = {\n",
    "                          'drd2': {\n",
    "                              'train_data': 'drd2_regression.train.csv',\n",
    "                              'test_data': 'drd2_regression.test.csv',\n",
    "                              'y_field': 'activity'\n",
    "                          }\n",
    "    }\n",
    "\n",
    "    train_data_file = usecase_params[usecase]['train_data']\n",
    "    test_data_file  = usecase_params[usecase]['test_data']\n",
    "    y_field = usecase_params[usecase]['y_field']\n",
    "    return train_data_file , test_data_file, y_field\n",
    "\n",
    "def load_config(acquisition,seed,loop, jobid=None):\n",
    "    if not jobid:\n",
    "        jobid=datetime.now().strftime(\"%d-%m-%Y\")\n",
    "    else:\n",
    "        jobid=jobid\n",
    "    jobname = 'Task2_demo_{}'.format(acquisition)\n",
    "\n",
    "    N0=10 # size of initial training data L\n",
    "    n_restarts=2 # parameters of GP optimization\n",
    "    n_batch=10    # number of molecules shown at each iteration\n",
    "    n_iteration=10  # the number of iteration\n",
    "    fpdim=1024 #dimension of morgan fingerprint\n",
    "    step=1\n",
    "    cwd=os.getcwd()\n",
    "    \n",
    "    reinvent_dir = os.path.expanduser(\"/path/to/Reinvent\")\n",
    "    reinvent_env = os.path.expanduser(\"/path/to/conda_environment/for/Reinvent\")\n",
    "    output_dir=os.path.join(cwd,\"./results/{}_{}_seed_{}\".format(jobname, jobid, seed))\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    output_dir=os.path.join(output_dir,\"./loop{}\".format(loop))\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    lastloop_dir=None\n",
    "    return jobid, jobname, N0, n_restarts, n_batch, n_iteration, fpdim, step, reinvent_dir, reinvent_env, output_dir, lastloop_dir\n",
    "\n",
    "def load_data_config(usecase, lastloop_dir=None):\n",
    "    train_num=10000 # sample 10.000 molecules from training dataset to U\n",
    "    test_num=2000 # sample 2000 molecules from testing dataset to the test set\n",
    "    data_dir= './data/'\n",
    "    train_data_file , test_data_file, y_field=parse_usecase(usecase)\n",
    "\n",
    "    test_data_path=os.path.join(data_dir,test_data_file) if test_data_file else None\n",
    "\n",
    "    if lastloop_dir:\n",
    "        train_data_path=os.path.join(lastloop_dir,train_data_file)\n",
    "    else:\n",
    "        train_data_path=os.path.join(data_dir,train_data_file)\n",
    "    return train_num, test_num, train_data_path, test_data_path, y_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_updated, y_updated, jobid, acquisition,seed,loop, iteration):\n",
    "    jobid, jobname, N0, n_restarts, n_batch, _, fpdim, step,\\\n",
    "    reinvent_dir, reinvent_env, output_dir, _ =load_config(acquisition,seed,loop, jobid)\n",
    "    start=time.time()\n",
    "    gpc=Tanimoto_model(X_updated, y_updated)\n",
    "    model_dir = output_dir+\"/models/model_{}\".format(iteration)\n",
    "    if (iteration % step)==0:\n",
    "        gpc.predict_y_compiled = tf.function(gpc.predict_f, input_signature=[tf.TensorSpec(shape=[None, fpdim], dtype=tf.float64)])\n",
    "    tf.saved_model.save(gpc, model_dir)\n",
    "    print('save model at path {}'.format(model_dir))\n",
    "    conf_filename = write_config_file(jobid, jobname, reinvent_dir, reinvent_env, output_dir, fpdim, loop, iteration, model_dir, seed)\n",
    "    write_sample_file(jobid, jobname, output_dir, loop, iteration)\n",
    "    print('training model at iteration {} use {} s'.format(iteration, time.time()-start))\n",
    "    return gpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(p):\n",
    "    label=p.copy()\n",
    "    label[label>0.5]=1\n",
    "    label[label<0.5]=0\n",
    "    label[label==0.5]=np.random.binomial(1, 0.5,sum(label==0.5))\n",
    "    return label.astype(int)\n",
    "\n",
    "def evaluate(model,X,Y,accuracy,recall, precision, f1, MCC, Kappa, rmse):\n",
    "    mean, var = model.predict_f(X)\n",
    "    rmse+= [np.sqrt(np.mean((mean-Y)**2))]\n",
    "    pred=get_labels(mean.numpy())\n",
    "    Y=get_labels(Y)\n",
    "    recall+=[recall_score(Y,pred)]\n",
    "    accuracy += [accuracy_score(Y, pred)]\n",
    "    precision+=[precision_score(Y,pred)]\n",
    "    f1+=[f1_score(Y,pred)]\n",
    "    MCC += [matthews_corrcoef(Y, pred)]\n",
    "    Kappa += [cohen_kappa_score(Y, pred)]\n",
    "    return accuracy, recall, precision, f1, MCC, Kappa, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HITL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configuration \n",
    "jobid, jobname, N0, n_restarts, n_batch, n_iteration, fpdim, step, reinvent_dir, reinvent_env, output_dir, lastloop_dir = load_config(acquisition,seed, loop, last_job)\n",
    "\n",
    "# load training data configuration\n",
    "train_num, test_num, train_data_path, test_data_path, y_field = load_data_config(usecase,lastloop_dir)\n",
    "\n",
    "# load data\n",
    "X_train, X_test, y_train, y_test, smiles_train, id_train, X_L, X_U, y_L, y_U, L , U = load_data(train_data_path, test_data_path, output_dir, train_num, N0, y_field, id_field=None, ext='csv', sampling=True, normalization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_updated=X_L\n",
    "y_updated=y_L\n",
    "# Fit model with D_0\n",
    "start=time.time()\n",
    "gpc=train(X_updated, y_updated, jobid, acquisition,seed, loop, 0)\n",
    "print(\"training spend {} s\".format(time.time()-start))\n",
    "\n",
    "# Evaluate performance in test data\n",
    "accuracy, recall, precision, f1, MCC , Kappa, rmse =[] ,[], [], [], [], [], []\n",
    "start=time.time()\n",
    "accuracy, recall, precision, f1, MCC , Kappa, rmse = evaluate(gpc, X_test, y_test, accuracy, recall, precision, f1, MCC, Kappa, rmse)\n",
    "print(\"evaluation spend {} s\".format(time.time()-start))\n",
    "if verbose:\n",
    "    print('accuracy is {}'.format(accuracy))\n",
    "    print('recall is {}'.format(recall))\n",
    "    print('MCC is {}'.format(MCC))\n",
    "    print('Kappa is {}'.format(Kappa))\n",
    "    print('rmse is {}'.format(rmse))\n",
    "\n",
    "# Baseline prediction: mean of training data\n",
    "pred_baseline = np.repeat(1, len(y_test))\n",
    "label_test=get_labels(y_test)\n",
    "accuracy_baseline = accuracy_score(label_test, pred_baseline)\n",
    "recall_baseline=recall_score(label_test, pred_baseline)\n",
    "MCC_baseline = matthews_corrcoef(label_test, pred_baseline)\n",
    "Kappa_baseline = cohen_kappa_score(label_test, pred_baseline)\n",
    "\n",
    "if verbose: \n",
    "    print(\"Baseline accuracy: {}\".format(accuracy_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start iteration\")\n",
    "positive_molecules_num=np.array([np.sum(y_updated)],dtype=np.int)\n",
    "idx_query=np.array([],dtype='i')\n",
    "all_smiles_query=[]\n",
    "D0_smiles = smiles_train[L]\n",
    "\n",
    "for iteration in np.arange(1,n_iteration+1):\n",
    "    # CREATE QUERY\n",
    "    print('it: ' + str(iteration))\n",
    "    queryfile_identifier = 'query_it{}'.format(iteration) \n",
    "    if not READ_ONLY:\n",
    "        # Elicit n_batch feedback\n",
    "        t = iteration\n",
    "        i_query = select_query(U, X_train, n_batch, gpc, acquisition, X_updated, y_updated, L, t)\n",
    "    elif READ_ONLY:\n",
    "        query_csv = pd.read_csv(output_dir + '/query/{}.csv'.format(queryfile_identifier))\n",
    "        sq = query_csv['SMILES']\n",
    "        i_query = np.array([list(smiles_train).index(s) for s in sq])\n",
    "\n",
    "    idx_query=np.append(idx_query,i_query)\n",
    "    U = np.setdiff1d(U,i_query)\n",
    "    if verbose:\n",
    "        print(\"Feedback on n={}\".format(i_query))\n",
    "        print(\"Size of pool: {}\".format(len(U)))\n",
    "\n",
    "    query_smiles =smiles_train[i_query]\n",
    "    all_smiles_query += [query_smiles]\n",
    "\n",
    "    # Write the query to a csv file\n",
    "    if not READ_ONLY:\n",
    "        write_query_to_csv(query_smiles, id_train, i_query, output_dir + '/query/{}.csv'.format(queryfile_identifier), output_dir)\n",
    "\n",
    "    if not simulated_human:\n",
    "        input(\"Press Enter to continue...\")\n",
    "    # READ RESPONSE QUERY\n",
    "    print(\"Read response at iteration {}\".format(iteration))\n",
    "    # Response\n",
    "    if simulated_human:\n",
    "        y_response = y_train[i_query]\n",
    "        unkown_idx=np.where(y_response==-1)[0] \n",
    "        if(len(unkown_idx)>0): # If simulated feedback not pre-computed\n",
    "              y_response[unkown_idx]=query(query_smiles[unkown_idx])\n",
    "              #update y_train\n",
    "              y_train[i_query[unkown_idx]]=y_response[unkown_idx]\n",
    "    else:\n",
    "        y_csv = pd.read_csv(output_dir + '/query/{}_with_ratings.csv'.format(queryfile_identifier)) \n",
    "        smiles_response = list(y_csv['SMILES'])\n",
    "        responses = y_csv['rating'] \n",
    "        order = [smiles_response.index(smiles_train[i]) for i in i_query]\n",
    "        y_response = np.array([responses[i] for i in order], dtype=float) # match responses using smiles\n",
    "        y_response=y_response.reshape(len(y_response),1)\n",
    "        # filter out molecules without feedback (rating=0)\n",
    "        got_feedback = y_response != 0\n",
    "        y_response = y_response[got_feedback]\n",
    "        i_query = i_query[np.squeeze(got_feedback)]\n",
    "        # parse numerical feedback from columns 1=0, 5=1 and the rest in between\n",
    "        y_response = (y_response - 1)/4.0\n",
    "        y_response=y_response.reshape(len(y_response),1)\n",
    "        if verbose:\n",
    "            print(\"response is\")\n",
    "            print(y_response)\n",
    "\n",
    "    # Fit a new model, evaluate performance in test data\n",
    "    X_updated = np.vstack((X_updated, X_train[i_query,:]))\n",
    "    y_updated = np.concatenate((y_updated, y_response))\n",
    "    gpc=train(X_updated, y_updated,jobid, acquisition,seed,loop, iteration)\n",
    "    positive_molecules_num=np.append(positive_molecules_num,np.sum(y_updated))\n",
    "    accuracy, recall, precision, f1, MCC , Kappa, rmse = evaluate(gpc, X_test, y_test, accuracy, recall, precision, f1, MCC, Kappa, rmse)\n",
    "\n",
    "    if verbose:\n",
    "        print(accuracy[iteration])\n",
    "        print(precision[iteration])\n",
    "        print(f1[iteration])\n",
    "        print(recall[iteration])\n",
    "        print(MCC[iteration])\n",
    "        print(Kappa[iteration])\n",
    "        print(rmse[iteration])\n",
    "\n",
    "\n",
    "\n",
    "dat_save = {\n",
    "    'hitl params': {'N0': N0, 'T': iteration, 'n_batch': n_batch, 'step': step, 'acquisition': acquisition},\n",
    "    'accuracy': accuracy,\n",
    "    'recall': recall,\n",
    "    'precision':precision,\n",
    "    'f1':f1,\n",
    "    'MCC': MCC,\n",
    "    'Kappa':Kappa,\n",
    "    'rmse':rmse,\n",
    "    'baseline accuracy': accuracy_baseline,\n",
    "    'baseline_recall':recall_baseline,\n",
    "    'baseline_MCC':MCC_baseline,\n",
    "    'baseline_Kappa':Kappa_baseline,\n",
    "    'idx_query':idx_query,\n",
    "    'smiles_D0': D0_smiles,\n",
    "    'smiles_query':all_smiles_query,\n",
    "    'positive_molecules_num':positive_molecules_num\n",
    "}\n",
    "filename = output_dir + '/log_{}_it{}.p'.format(acquisition,iteration)\n",
    "with open(filename , 'wb') as f:\n",
    "    pickle.dump(dat_save, f)\n",
    "\n",
    "if verbose:\n",
    "    print('accuracy is {}'.format(accuracy))\n",
    "    print('recall is {}'.format(recall))\n",
    "    print('MCC is {}'.format(MCC))\n",
    "    print('Kappa is {}'.format(Kappa))\n",
    "    print('rmse is {}'.format(rmse))\n",
    "\n",
    "# Create shell scripts for evaluating performance of Chemist's component as Reinvent scoring function: \n",
    "# Script for reinforcement learning training of agent at each iteration\n",
    "write_runs_sh(seed, output_dir, reinvent_env, reinvent_dir, step, n_iteration)\n",
    "# Script for sampling molecules to evaluate the quality of Reinvent output once the agents are trained\n",
    "write_run_sample(seed, output_dir, reinvent_env, reinvent_dir, step, n_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance of Chemist's component as predictive model in the test data\n",
    "plt.plot(MCC)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('MCC')\n",
    "plt.title(\"Chemist's-component performance in the test set (drd2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('recall')\n",
    "plt.title(\"Chemist's-component performance in the test set (drd2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(\"Chemist's-component performance in the test set (drd2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Kappa)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('Kappa')\n",
    "plt.title(\"Chemist's-component performance in the test set (drd2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemist-component",
   "language": "python",
   "name": "cc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
